최근에 LLM과 RAG 관련해서 공부할 일이 있었는데, 그때 이해했던 걸 토대로 간단하게 정리해 보는 글!<br><br>

먼저, **LLM**(Large Language Model, 대규모 언어 모델)은 [방대한 양의 데이터로 사전 학습된 초대형 딥 러닝 모델](https://aws.amazon.com/ko/what-is/large-language-model/)이다.

요즘 남녀노소 따질 것 없이 많이 쓰는 GPT, Gemini와 같은 AI 모델이 여기에 해당한다.

쉽게 말하자면, 로봇이 인간의 말하기 방식을 배워서 인간처럼 대화할 수 있는 모델이라고 할 수 있다.

어떤 질문을 하든, 어떻게 말을 하든, 언제나 나에게 맞추어 대답을 해주니 전능하게 느껴지기도 하는데,

LLM의 한계는 여기에서 발생한다.<br><br>

LLM의 설명에서도 알 수 있듯이 LLM은 '방대한 양의 데이터'를 학습한다. 즉, 비용이 많이 든다.

매번 학습해서 성능을 더 올리고 싶겠지만 이는 만만한 일이 아니다.

그리고 LLM은 언어 모델로서, 입력에 대해 항상 출력을 생성하도록 설계되어 있기 때문에

정보가 부족한 상황에서도 그럴듯한 답변을 만들어내는 '환각 현상'도 심심치 않게 발생한다.<br><br>

이러한 한계를 극복하기 위해 RAG라는 기술이 탄생했다.

**RAG**(Retrieval-Augmented Generation, **검색 증강 생성**)는 [LLM이 답변하기 전에, 외부의 신뢰할 수 있는 자료를 참조하여 더 적절한 답변을 할 수 있도록 하는 기술](https://aws.amazon.com/ko/what-is/retrieval-augmented-generation/)이다.

여기서 표현이 헷갈릴 수 있는데, 외부의 자료를 '학습'하는 것이 아니다. '참조'하는 것에 가깝다.

그렇기 때문에 비용은 줄이고, 답변의 질은 높일 수 있는 것이다.<br><br>

LLM에 RAG 기술을 활용하는 흐름을 간단히 살펴보면,

먼저, 데이터 전처리 과정을 거친다.

물론 참고 자료를 질문할때마다 첨부할 수도 있겠지만,

보통은 미리 데이터 전처리 과정을 거쳐 DB에 저장한 뒤에 필요한 때에 꺼내 쓴다.

그리고 사용자가 질문을 했을 때, 해당 질문을 분석하고 그 질문을 답하기 위해 필요한 정보를 DB에서 찾아온다.

그렇게 찾아온 정보를 조합해 질문에 대한 '참고 답안'을 만들고

LLM이 그 답안을 받아 비로소 적합한 답변이 만들어져 사용자에게 전달되는 것이다.<br><br>

정리해 보면, RAG는 LLM의 언어 생성 능력은 유지하면서 외부의 신뢰할 수 있는 데이터를 활용해

비용효율적으로 답변의 정확도를 높이는 기술이다.<br><br>

더더욱 쉽게 요리에 비유해 보면,

사용자는 요리를 주문하는 손님, LLM은 셰프, RAG는 재료를 준비해 주는 보조이다.

RAG가 주문서에 맞게 적절한 재료를 가져다주면 LLM은 손님이 먹기 좋게 요리해서 내보인다.<br><br>

RAG 과정을 많이 뭉뚱그려 표현하긴 했지만 이해에 도움이 되었기를 바라며.. 글을 마친다.
